{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-variable regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMH96wv1E6runn2sqbXMRSx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunkyu-Park/Deep-Learning-For-All/blob/main/Multi_variable_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qStV1nh4uKRP"
      },
      "source": [
        "# 모두를 위한 딥러닝 시즌 1 Version\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8inbjSgLuVz6"
      },
      "source": [
        "# Lab 4 Multi-variable linear regression\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x_data = [[73., 80., 75.],\n",
        "          [93., 88., 93.],\n",
        "          [89., 91., 90.],\n",
        "          [96., 98., 100.],\n",
        "          [73., 66., 70.]]\n",
        "y_data = [[152.],\n",
        "          [185.],\n",
        "          [180.],\n",
        "          [196.],\n",
        "          [142.]]\n",
        "\n",
        "tf.model = tf.keras.Sequential()\n",
        "\n",
        "tf.model.add(tf.keras.layers.Dense(units=1, input_dim=3))  # input_dim=3 gives multi-variable regression\n",
        "tf.model.add(tf.keras.layers.Activation('linear'))  # this line can be omitted, as linear activation is default\n",
        "# advanced reading https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6OaEOTwuguc",
        "outputId": "2c60b843-6dd4-42d1-e3c4-95dc084a8ce5"
      },
      "source": [
        "tf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5))\n",
        "tf.model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 4         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 4\n",
            "Trainable params: 4\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqaEUYELug1I",
        "outputId": "14e190c5-bde7-403d-efa3-bf6f7fd5f772"
      },
      "source": [
        "history = tf.model.fit(x_data, y_data, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 9.7831\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.7780\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.7728\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.7676\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.7624\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.7572\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.7520\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.7468\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.7417\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.7365\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.7313\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.7261\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.7210\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.7158\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.7107\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.7055\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.7003\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.6952\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6900\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.6849\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6797\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6746\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6695\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6643\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.6593\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.6541\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.6490\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6438\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.6387\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6336\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.6285\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.6234\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.6183\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6131\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.6080\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.6030\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5978\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 9.5928\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5876\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.5826\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5775\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.5724\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5673\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.5622\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5572\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5521\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5470\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.5420\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5369\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5318\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.5268\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5217\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5167\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5116\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.5066\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.5015\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.4965\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.4914\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.4864\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.4814\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.4763\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.4713\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 9.4663\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.4612\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.4562\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.4512\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.4462\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.4412\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.4362\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.4312\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.4261\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.4211\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.4161\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.4111\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.4061\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.4012\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.3962\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.3912\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.3862\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.3812\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3762\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3712\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3663\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.3613\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.3564\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3514\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3464\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.3415\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.3366\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.3316\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.3266\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.3217\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.3167\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.3118\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.3068\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.3019\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 9.2970\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.2920\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.2871\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.2822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaWufBVFunyB",
        "outputId": "7f619b03-14bf-429b-bbca-cc2585893265"
      },
      "source": [
        "y_predict = tf.model.predict(np.array([[72., 93., 90.]]))\n",
        "print(y_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[176.19037]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csMouIRmuo74",
        "outputId": "e6ee518d-dc29-425a-d10f-78ab2bcbf95d"
      },
      "source": [
        "y_predict = tf.model.predict(np.array([[73., 80., 75.]]))\n",
        "print(y_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[155.70543]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pp8vy9xuuK9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnY9hZBPu1gQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW8Mc5gAu1jI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUdiyH0Mu1mG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rP48Q0PAHgY"
      },
      "source": [
        "# 모두를 위한 딥러닝 시즌 2 Version\n",
        "\n",
        "##03 - Multi-variable Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIYEYFl2AMfP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb5jBVY9BByg"
      },
      "source": [
        "###Simple Example(2 variabels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDCJ2cfZAUY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a276891-c976-4d8c-fbaa-5900208d9783"
      },
      "source": [
        "x1_data = [1, 0, 3, 0, 5]\n",
        "x2_data = [0, 2, 0, 4, 0]\n",
        "y_data  = [1, 2, 3, 4, 5]\n",
        "\n",
        "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "b  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "\n",
        "learning_rate = tf.Variable(0.001)\n",
        "\n",
        "for i in range(1000+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
        "    W1.assign_sub(learning_rate * W1_grad)\n",
        "    W2.assign_sub(learning_rate * W2_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
        "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | 421.747559 |     1.0859 |    -9.2478 |   0.235725\n",
            "   50 | 176.917908 |     0.9395 |    -5.9345 |   1.179017\n",
            "  100 |  74.890381 |     0.7727 |    -3.7900 |   1.753182\n",
            "  150 |  32.193108 |     0.6330 |    -2.3991 |   2.100188\n",
            "  200 |  14.274767 |     0.5296 |    -1.4948 |   2.305742\n",
            "  250 |   6.734056 |     0.4586 |    -0.9051 |   2.422436\n",
            "  300 |   3.546256 |     0.4126 |    -0.5188 |   2.482974\n",
            "  350 |   2.186060 |     0.3847 |    -0.2644 |   2.507898\n",
            "  400 |   1.593866 |     0.3694 |    -0.0953 |   2.510215\n",
            "  450 |   1.324829 |     0.3626 |     0.0184 |   2.498211\n",
            "  500 |   1.192129 |     0.3615 |     0.0962 |   2.477190\n",
            "  550 |   1.117285 |     0.3641 |     0.1507 |   2.450548\n",
            "  600 |   1.067317 |     0.3691 |     0.1900 |   2.420474\n",
            "  650 |   1.028355 |     0.3757 |     0.2194 |   2.388376\n",
            "  700 |   0.994557 |     0.3832 |     0.2423 |   2.355160\n",
            "  750 |   0.963451 |     0.3913 |     0.2610 |   2.321408\n",
            "  800 |   0.933984 |     0.3996 |     0.2768 |   2.287493\n",
            "  850 |   0.905699 |     0.4082 |     0.2908 |   2.253657\n",
            "  900 |   0.878387 |     0.4168 |     0.3035 |   2.220051\n",
            "  950 |   0.851950 |     0.4254 |     0.3153 |   2.186771\n",
            " 1000 |   0.826328 |     0.4340 |     0.3265 |   2.153877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt9BS3kUBGFw"
      },
      "source": [
        "###Simple Example(2 variables with Matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_AGY8XCAhQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570d2610-c58e-47ed-9ea3-98c4ca4a39f7"
      },
      "source": [
        "x_data = [\n",
        "    [1., 0., 3., 0., 5.],\n",
        "    [0., 2., 0., 4., 0.]\n",
        "]\n",
        "y_data  = [1, 2, 3, 4, 5]\n",
        "\n",
        "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0))\n",
        "b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
        "\n",
        "learning_rate = tf.Variable(0.001)\n",
        "\n",
        "for i in range(1000+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "        W.assign_sub(learning_rate * W_grad)\n",
        "        b.assign_sub(learning_rate * b_grad)\n",
        "    \n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
        "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |  15.025357 |    -0.2622 |     0.2716 |  -0.202924\n",
            "   50 |   3.556374 |     0.3836 |     0.5187 |   0.038538\n",
            "  100 |   0.896526 |     0.6810 |     0.6672 |   0.157715\n",
            "  150 |   0.252234 |     0.8172 |     0.7582 |   0.216674\n",
            "  200 |   0.085124 |     0.8792 |     0.8150 |   0.245534\n",
            "  250 |   0.037438 |     0.9073 |     0.8511 |   0.259079\n",
            "  300 |   0.022157 |     0.9200 |     0.8743 |   0.264674\n",
            "  350 |   0.016597 |     0.9259 |     0.8895 |   0.266052\n",
            "  400 |   0.014273 |     0.9288 |     0.8997 |   0.265147\n",
            "  450 |   0.013128 |     0.9305 |     0.9067 |   0.262986\n",
            "  500 |   0.012444 |     0.9316 |     0.9116 |   0.260124\n",
            "  550 |   0.011951 |     0.9326 |     0.9152 |   0.256874\n",
            "  600 |   0.011542 |     0.9335 |     0.9179 |   0.253413\n",
            "  650 |   0.011174 |     0.9344 |     0.9201 |   0.249846\n",
            "  700 |   0.010830 |     0.9354 |     0.9219 |   0.246234\n",
            "  750 |   0.010501 |     0.9363 |     0.9235 |   0.242614\n",
            "  800 |   0.010184 |     0.9372 |     0.9249 |   0.239010\n",
            "  850 |   0.009877 |     0.9381 |     0.9262 |   0.235436\n",
            "  900 |   0.009580 |     0.9391 |     0.9274 |   0.231900\n",
            "  950 |   0.009292 |     0.9400 |     0.9286 |   0.228407\n",
            " 1000 |   0.009012 |     0.9409 |     0.9297 |   0.224960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoQsvkPJBVg4"
      },
      "source": [
        "###Hypothesis without b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G81Nd6sAdI2",
        "outputId": "c809b23a-86c2-41a0-ce9c-f8554947c016"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
        "x_data = [\n",
        "    [1., 1., 1., 1., 1.], # bias(b)\n",
        "    [1., 0., 3., 0., 5.], \n",
        "    [0., 2., 0., 4., 0.]\n",
        "]\n",
        "y_data  = [1, 2, 3, 4, 5]\n",
        "\n",
        "W = tf.Variable(tf.random.uniform((1, 3), -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "for i in range(1000+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.matmul(W, x_data) # b가 없다\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "    grads = tape.gradient(cost, [W])\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
        "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |  17.293497 |    -0.3668 |    -0.2739 |     0.1515\n",
            "   50 |   4.211652 |    -0.0998 |     0.3972 |     0.4532\n",
            "  100 |   1.102621 |     0.0346 |     0.7046 |     0.6362\n",
            "  150 |   0.319834 |     0.1031 |     0.8441 |     0.7492\n",
            "  200 |   0.105668 |     0.1383 |     0.9068 |     0.8199\n",
            "  250 |   0.040766 |     0.1562 |     0.9346 |     0.8647\n",
            "  300 |   0.018919 |     0.1651 |     0.9466 |     0.8934\n",
            "  350 |   0.010852 |     0.1691 |     0.9518 |     0.9121\n",
            "  400 |   0.007632 |     0.1703 |     0.9541 |     0.9243\n",
            "  450 |   0.006242 |     0.1700 |     0.9552 |     0.9324\n",
            "  500 |   0.005580 |     0.1688 |     0.9558 |     0.9380\n",
            "  550 |   0.005215 |     0.1671 |     0.9563 |     0.9418\n",
            "  600 |   0.004977 |     0.1651 |     0.9568 |     0.9446\n",
            "  650 |   0.004793 |     0.1630 |     0.9573 |     0.9466\n",
            "  700 |   0.004634 |     0.1607 |     0.9579 |     0.9482\n",
            "  750 |   0.004489 |     0.1584 |     0.9584 |     0.9495\n",
            "  800 |   0.004351 |     0.1561 |     0.9590 |     0.9506\n",
            "  850 |   0.004220 |     0.1538 |     0.9596 |     0.9516\n",
            "  900 |   0.004092 |     0.1515 |     0.9602 |     0.9525\n",
            "  950 |   0.003969 |     0.1493 |     0.9608 |     0.9533\n",
            " 1000 |   0.003850 |     0.1470 |     0.9614 |     0.9540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjQCZFmVBeoY"
      },
      "source": [
        "###Custom Gradient\n",
        "ㆍ tf.train.GradientDescentOptimizer(): optimizer  \n",
        "ㆍ optimizer.apply_gradients(): update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4V-dNBLAesm",
        "outputId": "f3195a49-fbd0-42f8-fb37-a3802229a71f"
      },
      "source": [
        "# Multi-variable linear regression (1)\n",
        "\n",
        "X = tf.constant([[1., 2.], \n",
        "                 [3., 4.]])\n",
        "y = tf.constant([[1.5], [3.5]])\n",
        "\n",
        "W = tf.Variable(tf.random.normal((2, 1)))\n",
        "b = tf.Variable(tf.random.normal((1,)))\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "n_epoch = 1000+1\n",
        "print(\"epoch | cost\")\n",
        "for i in range(n_epoch):\n",
        "    # Use tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = tf.matmul(X, W) + b\n",
        "        cost = tf.reduce_mean(tf.square(y_pred - y))\n",
        "\n",
        "    # calculates the gradients of the loss\n",
        "    grads = tape.gradient(cost, [W, b])\n",
        "    \n",
        "    # updates parameters (W and b)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch | cost\n",
            "    0 |   2.915952\n",
            "   50 |   0.020451\n",
            "  100 |   0.013983\n",
            "  150 |   0.009560\n",
            "  200 |   0.006536\n",
            "  250 |   0.004469\n",
            "  300 |   0.003055\n",
            "  350 |   0.002089\n",
            "  400 |   0.001428\n",
            "  450 |   0.000977\n",
            "  500 |   0.000668\n",
            "  550 |   0.000456\n",
            "  600 |   0.000312\n",
            "  650 |   0.000213\n",
            "  700 |   0.000146\n",
            "  750 |   0.000100\n",
            "  800 |   0.000068\n",
            "  850 |   0.000047\n",
            "  900 |   0.000032\n",
            "  950 |   0.000022\n",
            " 1000 |   0.000015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njs_IoUPB2hP"
      },
      "source": [
        "###Predicting exam score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG2727KuAgWW",
        "outputId": "5ba89d34-4c60-42b0-b8da-4e7c41393f0d"
      },
      "source": [
        "\n",
        "# data and label\n",
        "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
        "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
        "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
        "Y  = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# weights\n",
        "w1 = tf.Variable(10.)\n",
        "w2 = tf.Variable(10.)\n",
        "w3 = tf.Variable(10.)\n",
        "b  = tf.Variable(10.)\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    # calculates the gradients of the cost\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "    \n",
        "    # update w1,w2,w3 and b\n",
        "    w1.assign_sub(learning_rate * w1_grad)\n",
        "    w2.assign_sub(learning_rate * w2_grad)\n",
        "    w3.assign_sub(learning_rate * w3_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | 5793889.5000\n",
            "   50 |   64291.1562\n",
            "  100 |     715.2903\n",
            "  150 |       9.8461\n",
            "  200 |       2.0152\n",
            "  250 |       1.9252\n",
            "  300 |       1.9210\n",
            "  350 |       1.9177\n",
            "  400 |       1.9145\n",
            "  450 |       1.9114\n",
            "  500 |       1.9081\n",
            "  550 |       1.9050\n",
            "  600 |       1.9018\n",
            "  650 |       1.8986\n",
            "  700 |       1.8955\n",
            "  750 |       1.8923\n",
            "  800 |       1.8892\n",
            "  850 |       1.8861\n",
            "  900 |       1.8829\n",
            "  950 |       1.8798\n",
            " 1000 |       1.8767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lQjts3AB_KP"
      },
      "source": [
        "###Multi-variable Linear Regression(1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrnyUu9FAi8f",
        "outputId": "8f4f6b2a-0064-4437-ca7f-04446351f06c"
      },
      "source": [
        "# data and label\n",
        "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
        "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
        "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
        "Y  = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# random weights\n",
        "w1 = tf.Variable(tf.random.normal((1,)))\n",
        "w2 = tf.Variable(tf.random.normal((1,)))\n",
        "w3 = tf.Variable(tf.random.normal((1,)))\n",
        "b  = tf.Variable(tf.random.normal((1,)))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    # calculates the gradients of the cost\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "    \n",
        "    # update w1,w2,w3 and b\n",
        "    w1.assign_sub(learning_rate * w1_grad)\n",
        "    w2.assign_sub(learning_rate * w2_grad)\n",
        "    w3.assign_sub(learning_rate * w3_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |   65949.9219\n",
            "   50 |     739.8290\n",
            "  100 |      16.2391\n",
            "  150 |       8.1952\n",
            "  200 |       8.0912\n",
            "  250 |       8.0753\n",
            "  300 |       8.0604\n",
            "  350 |       8.0456\n",
            "  400 |       8.0308\n",
            "  450 |       8.0160\n",
            "  500 |       8.0013\n",
            "  550 |       7.9866\n",
            "  600 |       7.9719\n",
            "  650 |       7.9573\n",
            "  700 |       7.9427\n",
            "  750 |       7.9282\n",
            "  800 |       7.9137\n",
            "  850 |       7.8993\n",
            "  900 |       7.8848\n",
            "  950 |       7.8704\n",
            " 1000 |       7.8561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zRz2S5bCEuP"
      },
      "source": [
        "###Multi-variable Linear Regression(2) - with Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og6A6gO_Akd-",
        "outputId": "71843ac2-b032-4284-8c18-911273008c86"
      },
      "source": [
        "data = np.array([\n",
        "    # X1,   X2,    X3,   y\n",
        "    [ 73.,  80.,  75., 152. ],\n",
        "    [ 93.,  88.,  93., 185. ],\n",
        "    [ 89.,  91.,  90., 180. ],\n",
        "    [ 96.,  98., 100., 196. ],\n",
        "    [ 73.,  66.,  70., 142. ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal((3, 1)))\n",
        "b = tf.Variable(tf.random.normal((1,)))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X):\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "print(\"epoch | cost\")\n",
        "\n",
        "n_epochs = 2000\n",
        "for i in range(n_epochs+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "    # calculates the gradients of the loss\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "    # updates parameters (W and b)\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch | cost\n",
            "    0 |  7841.2275\n",
            "  100 |     6.0632\n",
            "  200 |     5.0808\n",
            "  300 |     5.0631\n",
            "  400 |     5.0455\n",
            "  500 |     5.0281\n",
            "  600 |     5.0107\n",
            "  700 |     4.9934\n",
            "  800 |     4.9763\n",
            "  900 |     4.9592\n",
            " 1000 |     4.9422\n",
            " 1100 |     4.9252\n",
            " 1200 |     4.9084\n",
            " 1300 |     4.8916\n",
            " 1400 |     4.8750\n",
            " 1500 |     4.8584\n",
            " 1600 |     4.8419\n",
            " 1700 |     4.8255\n",
            " 1800 |     4.8092\n",
            " 1900 |     4.7930\n",
            " 2000 |     4.7768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiOWuH6GAl22",
        "outputId": "890c49a8-e099-4c19-c538-1d745abfcc6d"
      },
      "source": [
        "W.numpy()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02344068],\n",
              "       [0.50658476],\n",
              "       [1.4545549 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XpxlfcVAnAO",
        "outputId": "121adb71-aca8-4829-927b-0dff6a66606a"
      },
      "source": [
        "b.numpy()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.4628913], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si8IqzMJAoUe",
        "outputId": "89703eff-e3da-485f-e0ba-0d8c744ade3b"
      },
      "source": [
        "tf.matmul(X, W) + b"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
              "array([[152.79247],\n",
              "       [183.49594],\n",
              "       [180.55826],\n",
              "       [198.814  ],\n",
              "       [138.4275 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3_CYq9ZCOs4"
      },
      "source": [
        "###Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myqeq1OsApIN",
        "outputId": "089f66ba-d093-47ad-a73d-123b5ecfedf3"
      },
      "source": [
        "Y # labels, 실제값"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[152.0, 185.0, 180.0, 196.0, 142.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhxmLG5eAq-2",
        "outputId": "b83ae6e6-6a41-400d-cb2e-1556c4985453"
      },
      "source": [
        "predict(X).numpy() # prediction, 예측값"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[152.79247],\n",
              "       [183.49594],\n",
              "       [180.55826],\n",
              "       [198.814  ],\n",
              "       [138.4275 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AqMS7vWAsIm",
        "outputId": "ead2bd8d-6ac3-468a-978a-1329fa5de807"
      },
      "source": [
        "# 새로운 데이터에 대한 예측\n",
        "\n",
        "predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()  "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[185.49371],\n",
              "       [173.67487]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc01P4T8AvO1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}